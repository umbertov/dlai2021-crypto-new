optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.SGD
  #  These are all default parameters for the Adam optimizer
  lr: 0.001
  weight_decay: 0

use_lr_scheduler: ${lr_scheduler.use_lr_scheduler}
lr_scheduler: ${lr_scheduler.scheduler}
